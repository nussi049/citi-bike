{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/analysis_bike_all_grid0250.duckdb\n",
      "GRID_DEG: 0.025 | FORCE_REBUILD: True | CELL_COVERAGE: 1.0\n",
      "Window: 2020-01-01 → 2026-01-01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# -----------------------------\n",
    "# INPUTS\n",
    "# -----------------------------\n",
    "TRIPS = PROJECT_ROOT / \"data\" / \"interim\" / \"tripdata_2013_2025_clean.parquet\"\n",
    "CRASH_BIKE = PROJECT_ROOT / \"data\" / \"processed\" / \"crashes_bike_liability.parquet\"\n",
    "WEATHER_HOURLY_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"weather_hourly_openmeteo\"\n",
    "\n",
    "# -----------------------------\n",
    "# OUTPUT DIR\n",
    "# -----------------------------\n",
    "OUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"risk_hourly_mc\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TMP_DIR = PROJECT_ROOT / \"duckdb_tmp\"\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# GRID + WINDOW\n",
    "# -----------------------------\n",
    "GRID_DEG = 0.025       \n",
    "TMIN = \"2020-01-01\"\n",
    "TMAX = \"2026-01-01\"\n",
    "\n",
    "DB_PATH = OUT_DIR / f\"analysis_bike_all_grid{int(GRID_DEG*10000):04d}.duckdb\"\n",
    "con = duckdb.connect(DB_PATH.as_posix())\n",
    "\n",
    "# Make DuckDB stable on laptop RAM\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "con.execute(\"PRAGMA preserve_insertion_order=false;\")\n",
    "con.execute(\"PRAGMA memory_limit='8GB';\")\n",
    "con.execute(f\"PRAGMA temp_directory='{TMP_DIR.as_posix()}';\")\n",
    "\n",
    "# Forcing rebuild prevents \"same fit\" due to cached files.\n",
    "FORCE_REBUILD = True\n",
    "\n",
    "# To make grid-model feasible: keep cells covering X% of total exposure (train window).\n",
    "CELL_COVERAGE = 1.0    # 0.95..0.995 (higher => more cells => slower)\n",
    "\n",
    "print(\"DuckDB:\", DB_PATH)\n",
    "print(\"GRID_DEG:\", GRID_DEG, \"| FORCE_REBUILD:\", FORCE_REBUILD, \"| CELL_COVERAGE:\", CELL_COVERAGE)\n",
    "print(\"Window:\", TMIN, \"→\", TMAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/crash_cell_hour.parquet\n",
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/crash_cell_hour.parquet\n",
      "       n              min_ts              max_ts\n",
      "0  43871 2020-01-01 10:00:00 2025-12-31 20:00:00\n"
     ]
    }
   ],
   "source": [
    "crash_cell_hour_path = OUT_DIR / \"crash_cell_hour.parquet\"\n",
    "\n",
    "if FORCE_REBUILD and crash_cell_hour_path.exists():\n",
    "    crash_cell_hour_path.unlink()\n",
    "    print(\"Deleted:\", crash_cell_hour_path)\n",
    "\n",
    "if crash_cell_hour_path.exists():\n",
    "    print(\"Exists, skipping:\", crash_cell_hour_path)\n",
    "else:\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      WITH base AS (\n",
    "        SELECT\n",
    "          -- Preferred: DATE + TIME (works if crash_date is DATE/TIMESTAMP and crash_time is TIME-like)\n",
    "          (try_cast(crash_date AS DATE) + try_cast(crash_time AS TIME))::TIMESTAMP AS crash_ts_dt,\n",
    "\n",
    "          -- Fallback: parse crash_time as \"H:MM\" / \"HH:MM\" strings\n",
    "          try_strptime(\n",
    "            strftime(try_cast(crash_date AS DATE), '%Y-%m-%d') || ' ' ||\n",
    "            printf(\n",
    "              '%02d:%02d',\n",
    "              try_cast(regexp_extract(CAST(crash_time AS VARCHAR), '^(\\\\d{{1,2}})', 1) AS INTEGER),\n",
    "              try_cast(regexp_extract(CAST(crash_time AS VARCHAR), ':(\\\\d{{2}})', 1) AS INTEGER)\n",
    "            ),\n",
    "            '%Y-%m-%d %H:%M'\n",
    "          ) AS crash_ts_str,\n",
    "\n",
    "          latitude,\n",
    "          longitude\n",
    "        FROM read_parquet('{CRASH_BIKE.as_posix()}')\n",
    "      ),\n",
    "      ts AS (\n",
    "        SELECT\n",
    "          COALESCE(crash_ts_dt, crash_ts_str) AS crash_ts,\n",
    "          latitude,\n",
    "          longitude\n",
    "        FROM base\n",
    "      ),\n",
    "      binned AS (\n",
    "        SELECT\n",
    "          date_trunc('hour', crash_ts) AS hour_ts,\n",
    "          floor(latitude  / {GRID_DEG}) * {GRID_DEG} AS grid_lat,\n",
    "          floor(longitude / {GRID_DEG}) * {GRID_DEG} AS grid_lng,\n",
    "          1 AS y_bike\n",
    "        FROM ts\n",
    "        WHERE crash_ts IS NOT NULL\n",
    "          AND latitude IS NOT NULL AND longitude IS NOT NULL\n",
    "          AND crash_ts >= TIMESTAMP '{TMIN}'\n",
    "          AND crash_ts <  TIMESTAMP '{TMAX}'\n",
    "      )\n",
    "      SELECT\n",
    "        hour_ts,\n",
    "        CAST(grid_lat AS DOUBLE) AS grid_lat,\n",
    "        CAST(grid_lng AS DOUBLE) AS grid_lng,\n",
    "        CAST(grid_lat AS VARCHAR) || '_' || CAST(grid_lng AS VARCHAR) AS cell_id,\n",
    "        SUM(y_bike) AS y_bike\n",
    "      FROM binned\n",
    "      GROUP BY 1,2,3,4\n",
    "    )\n",
    "    TO '{crash_cell_hour_path.as_posix()}'\n",
    "    (FORMAT PARQUET);\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Wrote:\", crash_cell_hour_path)\n",
    "\n",
    "# sanity\n",
    "print(con.execute(f\"\"\"\n",
    "SELECT COUNT(*) n, MIN(hour_ts) min_ts, MAX(hour_ts) max_ts\n",
    "FROM read_parquet('{crash_cell_hour_path.as_posix()}');\n",
    "\"\"\").fetch_df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/exposure_cell_hour.parquet\n",
      "Creating exposure (year-by-year) for 2020-01-01 to 2026-01-01...\n",
      "  Processing year 2020 (2020-01-01 to 2021-01-01)...\n",
      "  Processing year 2021 (2021-01-01 to 2022-01-01)...\n",
      "  Processing year 2022 (2022-01-01 to 2023-01-01)...\n",
      "  Processing year 2023 (2023-01-01 to 2024-01-01)...\n"
     ]
    }
   ],
   "source": [
    "exposure_cell_hour_path = OUT_DIR / \"exposure_cell_hour.parquet\"\n",
    "\n",
    "if FORCE_REBUILD and exposure_cell_hour_path.exists():\n",
    "    exposure_cell_hour_path.unlink()\n",
    "    print(\"Deleted:\", exposure_cell_hour_path)\n",
    "\n",
    "if exposure_cell_hour_path.exists():\n",
    "    print(\"Exists, skipping:\", exposure_cell_hour_path)\n",
    "else:\n",
    "    # Process year-by-year to avoid memory issues\n",
    "    print(f\"Creating exposure (year-by-year) for {TMIN} to {TMAX}...\")\n",
    "    \n",
    "    # Extract year range from TMIN/TMAX\n",
    "    import datetime\n",
    "    year_min = datetime.datetime.fromisoformat(TMIN).year\n",
    "    year_max = datetime.datetime.fromisoformat(TMAX).year\n",
    "    years = range(year_min, year_max + 1)\n",
    "    \n",
    "    temp_files = []\n",
    "    \n",
    "    for year in years:\n",
    "        year_start = max(f\"{year}-01-01\", TMIN)  # Don't go before TMIN\n",
    "        if year == year_max:\n",
    "            year_end = TMAX  # Use TMAX for last year\n",
    "        else:\n",
    "            year_end = f\"{year+1}-01-01\"\n",
    "        \n",
    "        temp_path = OUT_DIR / f\"_temp_exp_{year}.parquet\"\n",
    "        temp_files.append(temp_path)\n",
    "        \n",
    "        if temp_path.exists():\n",
    "            print(f\"  Year {year}: already exists, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing year {year} ({year_start} to {year_end})...\")\n",
    "        con.execute(f\"\"\"\n",
    "        COPY (\n",
    "          WITH trips AS (\n",
    "            SELECT\n",
    "              try_cast(started_at AS TIMESTAMP) AS started_at,\n",
    "              try_cast(ended_at   AS TIMESTAMP) AS ended_at,\n",
    "              start_lat, start_lng,\n",
    "              duration_sec\n",
    "            FROM read_parquet('{TRIPS.as_posix()}')\n",
    "            WHERE try_cast(started_at AS TIMESTAMP) >= TIMESTAMP '{year_start}'\n",
    "              AND try_cast(started_at AS TIMESTAMP) <  TIMESTAMP '{year_end}'\n",
    "              AND start_lat IS NOT NULL AND start_lng IS NOT NULL\n",
    "              AND ended_at IS NOT NULL\n",
    "              AND duration_sec IS NOT NULL\n",
    "              AND duration_sec > 0\n",
    "              AND duration_sec < 4*60*60\n",
    "          ),\n",
    "          binned AS (\n",
    "            SELECT\n",
    "              floor(start_lat / {GRID_DEG}) * {GRID_DEG} AS grid_lat,\n",
    "              floor(start_lng / {GRID_DEG}) * {GRID_DEG} AS grid_lng,\n",
    "              CAST(floor(start_lat / {GRID_DEG}) * {GRID_DEG} AS VARCHAR) || '_' ||\n",
    "              CAST(floor(start_lng / {GRID_DEG}) * {GRID_DEG} AS VARCHAR) AS cell_id,\n",
    "              started_at,\n",
    "              ended_at\n",
    "            FROM trips\n",
    "          ),\n",
    "          expanded AS (\n",
    "            SELECT\n",
    "              cell_id, grid_lat, grid_lng,\n",
    "              gs AS hour_ts,\n",
    "              started_at, ended_at\n",
    "            FROM binned\n",
    "            CROSS JOIN generate_series(\n",
    "              date_trunc('hour', started_at),\n",
    "              date_trunc('hour', ended_at),\n",
    "              INTERVAL '1 hour'\n",
    "            ) AS t(gs)\n",
    "          ),\n",
    "          overlap AS (\n",
    "            SELECT\n",
    "              cell_id, grid_lat, grid_lng, hour_ts,\n",
    "              GREATEST(\n",
    "                0,\n",
    "                EXTRACT(EPOCH FROM LEAST(ended_at, hour_ts + INTERVAL '1 hour')\n",
    "                              - GREATEST(started_at, hour_ts))\n",
    "              ) AS overlap_sec\n",
    "            FROM expanded\n",
    "          )\n",
    "          SELECT\n",
    "            hour_ts,\n",
    "            CAST(grid_lat AS DOUBLE) AS grid_lat,\n",
    "            CAST(grid_lng AS DOUBLE) AS grid_lng,\n",
    "            cell_id,\n",
    "            SUM(overlap_sec)/60.0 AS exposure_min\n",
    "          FROM overlap\n",
    "          WHERE overlap_sec > 0\n",
    "          GROUP BY 1,2,3,4\n",
    "        )\n",
    "        TO '{temp_path.as_posix()}'\n",
    "        (FORMAT PARQUET);\n",
    "        \"\"\")\n",
    "    \n",
    "    # Combine all years\n",
    "    print(\"  Combining years...\")\n",
    "    all_files = \"', '\".join([f.as_posix() for f in temp_files if f.exists()])\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      SELECT * FROM read_parquet(['{all_files}'])\n",
    "    )\n",
    "    TO '{exposure_cell_hour_path.as_posix()}'\n",
    "    (FORMAT PARQUET);\n",
    "    \"\"\")\n",
    "    \n",
    "    # Cleanup\n",
    "    for f in temp_files:\n",
    "        if f.exists():\n",
    "            f.unlink()\n",
    "    \n",
    "    print(\"Wrote:\", exposure_cell_hour_path)\n",
    "\n",
    "# sanity\n",
    "print(con.execute(f\"\"\"\n",
    "SELECT COUNT(*) n, MIN(hour_ts) min_ts, MAX(hour_ts) max_ts, SUM(exposure_min) total_exp_min\n",
    "FROM read_parquet('{exposure_cell_hour_path.as_posix()}');\n",
    "\"\"\").fetch_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/crashes_bike_hour_citywide.parquet\n",
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/exposure_hour_citywide.parquet\n",
      "26015 hours crashes citywide\n",
      "51763 hours exposure citywide\n"
     ]
    }
   ],
   "source": [
    "crashes_citywide_path  = OUT_DIR / \"crashes_bike_hour_citywide.parquet\"\n",
    "exposure_citywide_path = OUT_DIR / \"exposure_hour_citywide.parquet\"\n",
    "\n",
    "if FORCE_REBUILD:\n",
    "    if crashes_citywide_path.exists(): crashes_citywide_path.unlink()\n",
    "    if exposure_citywide_path.exists(): exposure_citywide_path.unlink()\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT hour_ts, SUM(y_bike) AS y_bike\n",
    "  FROM read_parquet('{crash_cell_hour_path.as_posix()}')\n",
    "  GROUP BY 1\n",
    "  ORDER BY 1\n",
    ")\n",
    "TO '{crashes_citywide_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  SELECT hour_ts, SUM(exposure_min) AS exposure_min\n",
    "  FROM read_parquet('{exposure_cell_hour_path.as_posix()}')\n",
    "  GROUP BY 1\n",
    "  ORDER BY 1\n",
    ")\n",
    "TO '{exposure_citywide_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote:\", crashes_citywide_path)\n",
    "print(\"Wrote:\", exposure_citywide_path)\n",
    "print(con.execute(f\"SELECT COUNT(*) FROM read_parquet('{crashes_citywide_path.as_posix()}')\").fetchone()[0],\n",
    "      \"hours crashes citywide\")\n",
    "print(con.execute(f\"SELECT COUNT(*) FROM read_parquet('{exposure_citywide_path.as_posix()}')\").fetchone()[0],\n",
    "      \"hours exposure citywide\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather_hourly rows: 52561\n",
      "weather range: (datetime.datetime(2020, 1, 1, 0, 0), datetime.datetime(2025, 12, 30, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW weather_hourly AS\n",
    "SELECT\n",
    "  CAST(timestamp AS TIMESTAMP) AS hour_ts,\n",
    "  temp, prcp, snow, wspd\n",
    "FROM read_parquet('{(WEATHER_HOURLY_DIR.as_posix() + \"/**/*.parquet\")}')\n",
    "WHERE hour_ts >= TIMESTAMP '{TMIN}'\n",
    "  AND hour_ts <  TIMESTAMP '{TMAX}';\n",
    "\"\"\")\n",
    "\n",
    "print(\"weather_hourly rows:\", con.execute(\"SELECT COUNT(*) FROM weather_hourly\").fetchone()[0])\n",
    "print(\"weather range:\", con.execute(\"SELECT MIN(hour_ts), MAX(hour_ts) FROM weather_hourly\").fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/city_train_bike_all_2020_2024.parquet rows= 43752\n",
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/city_2025_bike_all.parquet rows= 8017\n"
     ]
    }
   ],
   "source": [
    "city_train_path = OUT_DIR / \"city_train_bike_all_2020_2024.parquet\"\n",
    "city_2025_path  = OUT_DIR / \"city_2025_bike_all.parquet\"\n",
    "\n",
    "if FORCE_REBUILD:\n",
    "    if city_train_path.exists(): city_train_path.unlink()\n",
    "    if city_2025_path.exists(): city_2025_path.unlink()\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  WITH e AS (\n",
    "    SELECT hour_ts, exposure_min\n",
    "    FROM read_parquet('{exposure_citywide_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2020-01-01' AND hour_ts < TIMESTAMP '2025-01-01'\n",
    "  ),\n",
    "  c AS (\n",
    "    SELECT hour_ts, y_bike\n",
    "    FROM read_parquet('{crashes_citywide_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2020-01-01' AND hour_ts < TIMESTAMP '2025-01-01'\n",
    "  )\n",
    "  SELECT\n",
    "    e.hour_ts,\n",
    "    e.exposure_min,\n",
    "    COALESCE(c.y_bike, 0) AS y_bike,\n",
    "    EXTRACT(HOUR  FROM e.hour_ts) AS hour_of_day,\n",
    "    EXTRACT(DOW   FROM e.hour_ts) AS dow,\n",
    "    EXTRACT(MONTH FROM e.hour_ts) AS month,\n",
    "    w.temp, w.prcp, w.snow, w.wspd\n",
    "  FROM e\n",
    "  LEFT JOIN c USING(hour_ts)\n",
    "  LEFT JOIN weather_hourly w USING(hour_ts)\n",
    "  WHERE e.exposure_min > 0\n",
    "  ORDER BY e.hour_ts\n",
    ")\n",
    "TO '{city_train_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  WITH e AS (\n",
    "    SELECT hour_ts, exposure_min\n",
    "    FROM read_parquet('{exposure_citywide_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2025-01-01' AND hour_ts < TIMESTAMP '2026-01-01'\n",
    "  )\n",
    "  SELECT\n",
    "    e.hour_ts,\n",
    "    e.exposure_min,\n",
    "    EXTRACT(HOUR  FROM e.hour_ts) AS hour_of_day,\n",
    "    EXTRACT(DOW   FROM e.hour_ts) AS dow,\n",
    "    EXTRACT(MONTH FROM e.hour_ts) AS month,\n",
    "    w.temp, w.prcp, w.snow, w.wspd\n",
    "  FROM e\n",
    "  LEFT JOIN weather_hourly w USING(hour_ts)\n",
    "  WHERE e.exposure_min > 0\n",
    "  ORDER BY e.hour_ts\n",
    ")\n",
    "TO '{city_2025_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote:\", city_train_path, \"rows=\", con.execute(f\"SELECT COUNT(*) FROM read_parquet('{city_train_path.as_posix()}')\").fetchone()[0])\n",
    "print(\"Wrote:\", city_2025_path,  \"rows=\", con.execute(f\"SELECT COUNT(*) FROM read_parquet('{city_2025_path.as_posix()}')\").fetchone()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/grid_cells_keep_2020_2024.parquet\n",
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/grid_train_cell_hour_2020_2024.parquet\n",
      "         n    y_mean     exp_mean\n",
      "0  1818038  0.014245  1211.783519\n"
     ]
    }
   ],
   "source": [
    "grid_train_path = OUT_DIR / \"grid_train_cell_hour_2020_2024.parquet\"\n",
    "cells_keep_path = OUT_DIR / \"grid_cells_keep_2020_2024.parquet\"\n",
    "\n",
    "if FORCE_REBUILD:\n",
    "    if grid_train_path.exists(): grid_train_path.unlink()\n",
    "    if cells_keep_path.exists(): cells_keep_path.unlink()\n",
    "\n",
    "# 1) Determine top exposure cells covering CELL_COVERAGE of total exposure\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  WITH cell_exp AS (\n",
    "    SELECT\n",
    "      cell_id,\n",
    "      SUM(exposure_min) AS exp_sum\n",
    "    FROM read_parquet('{exposure_cell_hour_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2020-01-01'\n",
    "      AND hour_ts <  TIMESTAMP '2025-01-01'\n",
    "    GROUP BY 1\n",
    "  ),\n",
    "  ranked AS (\n",
    "    SELECT\n",
    "      cell_id,\n",
    "      exp_sum,\n",
    "      SUM(exp_sum) OVER (ORDER BY exp_sum DESC) AS cum_exp,\n",
    "      SUM(exp_sum) OVER () AS total_exp\n",
    "    FROM cell_exp\n",
    "  )\n",
    "  SELECT\n",
    "    cell_id,\n",
    "    exp_sum,\n",
    "    cum_exp / NULLIF(total_exp,0) AS cum_share\n",
    "  FROM ranked\n",
    "  WHERE cum_exp / NULLIF(total_exp,0) <= {CELL_COVERAGE}\n",
    ")\n",
    "TO '{cells_keep_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote:\", cells_keep_path)\n",
    "\n",
    "# 2) Build grid training mart: one row per (cell_id, hour_ts)\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  WITH e AS (\n",
    "    SELECT\n",
    "      hour_ts, cell_id, grid_lat, grid_lng, exposure_min\n",
    "    FROM read_parquet('{exposure_cell_hour_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2020-01-01'\n",
    "      AND hour_ts <  TIMESTAMP '2025-01-01'\n",
    "      AND exposure_min > 0\n",
    "  ),\n",
    "  e_keep AS (\n",
    "    SELECT e.*\n",
    "    FROM e\n",
    "    INNER JOIN read_parquet('{cells_keep_path.as_posix()}') k USING(cell_id)\n",
    "  ),\n",
    "  c AS (\n",
    "    SELECT hour_ts, cell_id, y_bike\n",
    "    FROM read_parquet('{crash_cell_hour_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2020-01-01'\n",
    "      AND hour_ts <  TIMESTAMP '2025-01-01'\n",
    "  )\n",
    "  SELECT\n",
    "    e_keep.hour_ts,\n",
    "    e_keep.cell_id,\n",
    "    e_keep.grid_lat,\n",
    "    e_keep.grid_lng,\n",
    "    e_keep.exposure_min,\n",
    "    COALESCE(c.y_bike, 0) AS y_bike,\n",
    "    EXTRACT(HOUR  FROM e_keep.hour_ts) AS hour_of_day,\n",
    "    EXTRACT(DOW   FROM e_keep.hour_ts) AS dow,\n",
    "    EXTRACT(MONTH FROM e_keep.hour_ts) AS month,\n",
    "    w.temp, w.prcp, w.snow, w.wspd\n",
    "  FROM e_keep\n",
    "  LEFT JOIN c USING(hour_ts, cell_id)\n",
    "  LEFT JOIN weather_hourly w USING(hour_ts)\n",
    ")\n",
    "TO '{grid_train_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote:\", grid_train_path)\n",
    "print(con.execute(f\"\"\"\n",
    "SELECT COUNT(*) n, AVG(y_bike) y_mean, AVG(exposure_min) exp_mean\n",
    "FROM read_parquet('{grid_train_path.as_posix()}');\n",
    "\"\"\").fetch_df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula: y_bike ~ C(hour_of_day) + C(dow) + C(month) + grid_lat_norm + grid_lng_norm + lat2 + lng2 + lat_lng + temp + prcp + snow + wspd\n",
      "\n",
      "=== POISSON (grid model) ===\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_bike   No. Observations:              1477060\n",
      "Model:                            GLM   Df Residuals:                  1477010\n",
      "Model Family:                 Poisson   Df Model:                           49\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.2025e+05\n",
      "Date:                Tue, 30 Dec 2025   Deviance:                   1.9218e+05\n",
      "Time:                        17:10:10   Pearson chi2:                 2.14e+06\n",
      "No. Iterations:                     8   Pseudo R-squ. (CS):            0.01077\n",
      "Covariance Type:                  HC0                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept              -10.6977      0.052   -204.456      0.000     -10.800     -10.595\n",
      "C(hour_of_day)[T.1]     -0.4796      0.065     -7.385      0.000      -0.607      -0.352\n",
      "C(hour_of_day)[T.2]     -0.6405      0.083     -7.761      0.000      -0.802      -0.479\n",
      "C(hour_of_day)[T.3]     -0.4017      0.094     -4.285      0.000      -0.585      -0.218\n",
      "C(hour_of_day)[T.4]     -0.5251      0.107     -4.919      0.000      -0.734      -0.316\n",
      "C(hour_of_day)[T.5]     -0.6202      0.089     -7.002      0.000      -0.794      -0.447\n",
      "C(hour_of_day)[T.6]     -0.9875      0.068    -14.562      0.000      -1.120      -0.855\n",
      "C(hour_of_day)[T.7]     -1.2045      0.055    -21.936      0.000      -1.312      -1.097\n",
      "C(hour_of_day)[T.8]     -1.2120      0.049    -24.882      0.000      -1.307      -1.117\n",
      "C(hour_of_day)[T.9]     -1.1975      0.050    -23.975      0.000      -1.295      -1.100\n",
      "C(hour_of_day)[T.10]    -1.0845      0.050    -21.868      0.000      -1.182      -0.987\n",
      "C(hour_of_day)[T.11]    -1.0061      0.047    -21.246      0.000      -1.099      -0.913\n",
      "C(hour_of_day)[T.12]    -0.9639      0.046    -21.119      0.000      -1.053      -0.874\n",
      "C(hour_of_day)[T.13]    -0.9828      0.045    -22.019      0.000      -1.070      -0.895\n",
      "C(hour_of_day)[T.14]    -0.9968      0.043    -22.970      0.000      -1.082      -0.912\n",
      "C(hour_of_day)[T.15]    -1.0032      0.042    -23.610      0.000      -1.086      -0.920\n",
      "C(hour_of_day)[T.16]    -0.9249      0.041    -22.416      0.000      -1.006      -0.844\n",
      "C(hour_of_day)[T.17]    -0.9778      0.041    -23.974      0.000      -1.058      -0.898\n",
      "C(hour_of_day)[T.18]    -1.0167      0.041    -24.694      0.000      -1.097      -0.936\n",
      "C(hour_of_day)[T.19]    -0.8686      0.042    -20.827      0.000      -0.950      -0.787\n",
      "C(hour_of_day)[T.20]    -0.7115      0.043    -16.525      0.000      -0.796      -0.627\n",
      "C(hour_of_day)[T.21]    -0.5798      0.045    -12.917      0.000      -0.668      -0.492\n",
      "C(hour_of_day)[T.22]    -0.5639      0.047    -12.070      0.000      -0.656      -0.472\n",
      "C(hour_of_day)[T.23]    -0.6650      0.052    -12.903      0.000      -0.766      -0.564\n",
      "C(dow)[T.1]              0.2500      0.026      9.554      0.000       0.199       0.301\n",
      "C(dow)[T.2]              0.3289      0.026     12.893      0.000       0.279       0.379\n",
      "C(dow)[T.3]              0.3386      0.025     13.361      0.000       0.289       0.388\n",
      "C(dow)[T.4]              0.3487      0.025     13.749      0.000       0.299       0.398\n",
      "C(dow)[T.5]              0.3264      0.025     13.046      0.000       0.277       0.375\n",
      "C(dow)[T.6]              0.0601      0.025      2.359      0.018       0.010       0.110\n",
      "C(month)[T.2]            0.0257      0.044      0.580      0.562      -0.061       0.113\n",
      "C(month)[T.3]            0.0055      0.041      0.134      0.893      -0.075       0.086\n",
      "C(month)[T.4]           -0.1629      0.042     -3.864      0.000      -0.245      -0.080\n",
      "C(month)[T.5]           -0.1638      0.044     -3.725      0.000      -0.250      -0.078\n",
      "C(month)[T.6]           -0.2170      0.048     -4.479      0.000      -0.312      -0.122\n",
      "C(month)[T.7]           -0.2081      0.052     -3.971      0.000      -0.311      -0.105\n",
      "C(month)[T.8]           -0.2025      0.051     -4.009      0.000      -0.301      -0.103\n",
      "C(month)[T.9]           -0.1462      0.046     -3.153      0.002      -0.237      -0.055\n",
      "C(month)[T.10]          -0.1395      0.042     -3.309      0.001      -0.222      -0.057\n",
      "C(month)[T.11]          -0.0868      0.040     -2.150      0.032      -0.166      -0.008\n",
      "C(month)[T.12]           0.0281      0.041      0.688      0.492      -0.052       0.108\n",
      "grid_lat_norm           -0.3264      0.009    -37.512      0.000      -0.343      -0.309\n",
      "grid_lng_norm            0.6686      0.010     69.818      0.000       0.650       0.687\n",
      "lat2                     0.4295      0.010     42.561      0.000       0.410       0.449\n",
      "lng2                     0.0110      0.009      1.232      0.218      -0.006       0.028\n",
      "lat_lng                  0.0397      0.014      2.909      0.004       0.013       0.067\n",
      "temp                    -0.0855      0.016     -5.468      0.000      -0.116      -0.055\n",
      "prcp                     0.0264      0.006      4.316      0.000       0.014       0.038\n",
      "snow                     0.0076      0.007      1.058      0.290      -0.006       0.022\n",
      "wspd                     0.0095      0.007      1.341      0.180      -0.004       0.023\n",
      "========================================================================================\n",
      "\n",
      "Poisson overdispersion χ²/df: 1.4517294400296865\n",
      "\n",
      "=== NEG BIN (grid model) ===\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_bike   No. Observations:              1477060\n",
      "Model:                            GLM   Df Residuals:                  1477010\n",
      "Model Family:        NegativeBinomial   Df Model:                           49\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.2521e+05\n",
      "Date:                Tue, 30 Dec 2025   Deviance:                       83269.\n",
      "Time:                        17:10:54   Pearson chi2:                 1.54e+06\n",
      "No. Iterations:                    11   Pseudo R-squ. (CS):           0.006531\n",
      "Covariance Type:                  HC0                                         \n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept              -10.6197      0.057   -185.711      0.000     -10.732     -10.508\n",
      "C(hour_of_day)[T.1]     -0.5389      0.069     -7.806      0.000      -0.674      -0.404\n",
      "C(hour_of_day)[T.2]     -0.7146      0.087     -8.250      0.000      -0.884      -0.545\n",
      "C(hour_of_day)[T.3]     -0.4470      0.099     -4.527      0.000      -0.641      -0.253\n",
      "C(hour_of_day)[T.4]     -0.5932      0.111     -5.361      0.000      -0.810      -0.376\n",
      "C(hour_of_day)[T.5]     -0.6802      0.092     -7.422      0.000      -0.860      -0.501\n",
      "C(hour_of_day)[T.6]     -1.0362      0.071    -14.518      0.000      -1.176      -0.896\n",
      "C(hour_of_day)[T.7]     -1.2466      0.059    -21.201      0.000      -1.362      -1.131\n",
      "C(hour_of_day)[T.8]     -1.2287      0.053    -23.057      0.000      -1.333      -1.124\n",
      "C(hour_of_day)[T.9]     -1.2175      0.054    -22.403      0.000      -1.324      -1.111\n",
      "C(hour_of_day)[T.10]    -1.1255      0.054    -20.903      0.000      -1.231      -1.020\n",
      "C(hour_of_day)[T.11]    -1.0181      0.052    -19.627      0.000      -1.120      -0.916\n",
      "C(hour_of_day)[T.12]    -0.9769      0.050    -19.527      0.000      -1.075      -0.879\n",
      "C(hour_of_day)[T.13]    -0.9855      0.049    -20.067      0.000      -1.082      -0.889\n",
      "C(hour_of_day)[T.14]    -0.9967      0.048    -20.812      0.000      -1.091      -0.903\n",
      "C(hour_of_day)[T.15]    -0.9955      0.047    -21.221      0.000      -1.087      -0.904\n",
      "C(hour_of_day)[T.16]    -0.9079      0.046    -19.842      0.000      -0.998      -0.818\n",
      "C(hour_of_day)[T.17]    -0.9400      0.046    -20.606      0.000      -1.029      -0.851\n",
      "C(hour_of_day)[T.18]    -0.9656      0.046    -20.920      0.000      -1.056      -0.875\n",
      "C(hour_of_day)[T.19]    -0.8465      0.047    -18.201      0.000      -0.938      -0.755\n",
      "C(hour_of_day)[T.20]    -0.6982      0.048    -14.554      0.000      -0.792      -0.604\n",
      "C(hour_of_day)[T.21]    -0.5741      0.050    -11.568      0.000      -0.671      -0.477\n",
      "C(hour_of_day)[T.22]    -0.5688      0.052    -11.031      0.000      -0.670      -0.468\n",
      "C(hour_of_day)[T.23]    -0.6930      0.056    -12.405      0.000      -0.802      -0.583\n",
      "C(dow)[T.1]              0.2171      0.029      7.472      0.000       0.160       0.274\n",
      "C(dow)[T.2]              0.2918      0.028     10.307      0.000       0.236       0.347\n",
      "C(dow)[T.3]              0.3120      0.028     11.096      0.000       0.257       0.367\n",
      "C(dow)[T.4]              0.3164      0.028     11.232      0.000       0.261       0.372\n",
      "C(dow)[T.5]              0.2964      0.028     10.617      0.000       0.242       0.351\n",
      "C(dow)[T.6]              0.0615      0.029      2.130      0.033       0.005       0.118\n",
      "C(month)[T.2]            0.0328      0.048      0.681      0.496      -0.062       0.127\n",
      "C(month)[T.3]            0.0221      0.045      0.493      0.622      -0.066       0.110\n",
      "C(month)[T.4]           -0.1487      0.046     -3.235      0.001      -0.239      -0.059\n",
      "C(month)[T.5]           -0.1342      0.048     -2.774      0.006      -0.229      -0.039\n",
      "C(month)[T.6]           -0.2144      0.053     -4.023      0.000      -0.319      -0.110\n",
      "C(month)[T.7]           -0.2318      0.057     -4.033      0.000      -0.344      -0.119\n",
      "C(month)[T.8]           -0.2187      0.056     -3.926      0.000      -0.328      -0.110\n",
      "C(month)[T.9]           -0.1314      0.051     -2.578      0.010      -0.231      -0.031\n",
      "C(month)[T.10]          -0.1155      0.046     -2.503      0.012      -0.206      -0.025\n",
      "C(month)[T.11]          -0.0548      0.044     -1.249      0.212      -0.141       0.031\n",
      "C(month)[T.12]           0.0377      0.044      0.852      0.394      -0.049       0.124\n",
      "grid_lat_norm           -0.3731      0.009    -40.801      0.000      -0.391      -0.355\n",
      "grid_lng_norm            0.6733      0.010     67.666      0.000       0.654       0.693\n",
      "lat2                     0.4503      0.011     42.359      0.000       0.429       0.471\n",
      "lng2                     0.0059      0.010      0.620      0.536      -0.013       0.025\n",
      "lat_lng                  0.0805      0.015      5.527      0.000       0.052       0.109\n",
      "temp                    -0.0694      0.017     -4.025      0.000      -0.103      -0.036\n",
      "prcp                     0.0321      0.007      4.411      0.000       0.018       0.046\n",
      "snow                     0.0095      0.009      1.035      0.301      -0.008       0.027\n",
      "wspd                     0.0137      0.008      1.727      0.084      -0.002       0.029\n",
      "========================================================================================\n",
      "\n",
      "NB alpha used: 27.3052662747627\n",
      "\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/model_meta_bike_all.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "TARGET = \"y_bike\"\n",
    "MIN_EXPOSURE_MIN = 50.0\n",
    "EPS = 1e-6\n",
    "weather_cols = [\"temp\",\"prcp\",\"snow\",\"wspd\"]\n",
    "\n",
    "# Load (grid train) to pandas\n",
    "train_df = con.execute(f\"SELECT * FROM read_parquet('{grid_train_path.as_posix()}')\").fetch_df()\n",
    "\n",
    "# clean\n",
    "need = [\"exposure_min\", TARGET, \"hour_of_day\",\"dow\",\"month\",\"grid_lat\",\"grid_lng\"] + weather_cols\n",
    "train_df = train_df.dropna(subset=need).copy()\n",
    "train_df = train_df[train_df[\"exposure_min\"] >= MIN_EXPOSURE_MIN].copy()\n",
    "\n",
    "# offset\n",
    "train_df[\"log_exposure\"] = np.log(train_df[\"exposure_min\"].values + EPS)\n",
    "\n",
    "# categories\n",
    "train_df[\"hour_of_day\"] = train_df[\"hour_of_day\"].astype(int).astype(\"category\")\n",
    "train_df[\"dow\"] = train_df[\"dow\"].astype(int).astype(\"category\")\n",
    "train_df[\"month\"] = train_df[\"month\"].astype(int).astype(\"category\")\n",
    "\n",
    "weather_stats = {}\n",
    "for col in weather_cols:\n",
    "    m = float(train_df[col].mean())\n",
    "    s = float(train_df[col].std())\n",
    "    if s == 0 or np.isnan(s):\n",
    "        train_df.drop(columns=[col], inplace=True)\n",
    "    else:\n",
    "        weather_stats[col] = (m, s)\n",
    "        train_df[col] = (train_df[col] - m) / s\n",
    "\n",
    "active_weather_cols = [c for c in weather_cols if c in train_df.columns]\n",
    "\n",
    "# WICHTIG: Normalize spatial features too!\n",
    "lat_mean, lat_std = train_df[\"grid_lat\"].mean(), train_df[\"grid_lat\"].std()\n",
    "lng_mean, lng_std = train_df[\"grid_lng\"].mean(), train_df[\"grid_lng\"].std()\n",
    "\n",
    "train_df[\"grid_lat_norm\"] = (train_df[\"grid_lat\"] - lat_mean) / lat_std\n",
    "train_df[\"grid_lng_norm\"] = (train_df[\"grid_lng\"] - lng_mean) / lng_std\n",
    "\n",
    "# spatial features (normalized versions)\n",
    "train_df[\"lat2\"] = train_df[\"grid_lat_norm\"]**2\n",
    "train_df[\"lng2\"] = train_df[\"grid_lng_norm\"]**2\n",
    "train_df[\"lat_lng\"] = train_df[\"grid_lat_norm\"] * train_df[\"grid_lng_norm\"]\n",
    "\n",
    "# model formula (GRID MODEL) - use normalized versions\n",
    "rhs = [\n",
    "    \"C(hour_of_day)\", \"C(dow)\", \"C(month)\",\n",
    "    \"grid_lat_norm\", \"grid_lng_norm\", \"lat2\", \"lng2\", \"lat_lng\"\n",
    "] + active_weather_cols\n",
    "\n",
    "formula = f\"{TARGET} ~ \" + \" + \".join(rhs)\n",
    "print(\"Formula:\", formula)\n",
    "\n",
    "# ---- Poisson GLM ----\n",
    "poisson_model = smf.glm(\n",
    "    formula=formula,\n",
    "    data=train_df,\n",
    "    family=sm.families.Poisson(),\n",
    "    offset=train_df[\"log_exposure\"]\n",
    ")\n",
    "poisson_res = poisson_model.fit(cov_type=\"HC0\")\n",
    "print(\"\\n=== POISSON (grid model) ===\")\n",
    "print(poisson_res.summary())\n",
    "\n",
    "pearson_chi2 = float(np.sum(poisson_res.resid_pearson**2))\n",
    "disp_poiss = pearson_chi2 / float(poisson_res.df_resid)\n",
    "print(\"\\nPoisson overdispersion χ²/df:\", disp_poiss)\n",
    "\n",
    "# ---- Negative Binomial GLM (fixed alpha heuristic) ----\n",
    "mu_bar = float(poisson_res.mu.mean())\n",
    "alpha_init = max((disp_poiss - 1.0) / max(mu_bar, 1e-9), 1e-8)\n",
    "\n",
    "nb_model = smf.glm(\n",
    "    formula=formula,\n",
    "    data=train_df,\n",
    "    family=sm.families.NegativeBinomial(alpha=alpha_init),\n",
    "    offset=train_df[\"log_exposure\"]\n",
    ")\n",
    "nb_res = nb_model.fit(cov_type=\"HC0\")\n",
    "\n",
    "print(\"\\n=== NEG BIN (grid model) ===\")\n",
    "print(nb_res.summary())\n",
    "print(\"\\nNB alpha used:\", float(nb_res.model.family.alpha))\n",
    "\n",
    "meta = {\n",
    "    \"GRID_DEG\": GRID_DEG,\n",
    "    \"CELL_COVERAGE\": CELL_COVERAGE,\n",
    "    \"weather_stats\": {k: {\"mean\": v[0], \"std\": v[1]} for k,v in weather_stats.items()},\n",
    "    \"spatial_stats\": {\n",
    "        \"lat_mean\": float(lat_mean),\n",
    "        \"lat_std\": float(lat_std),\n",
    "        \"lng_mean\": float(lng_mean),\n",
    "        \"lng_std\": float(lng_std)\n",
    "    },\n",
    "    \"formula\": formula,\n",
    "    \"active_weather_cols\": active_weather_cols\n",
    "}\n",
    "meta_path = OUT_DIR / \"model_meta_bike_all.json\"\n",
    "meta_path.write_text(json.dumps(meta, indent=2))\n",
    "print(\"\\nSaved:\", meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPATIAL FEATURES DEBUG\n",
      "======================================================================\n",
      "\n",
      "Grid configuration:\n",
      "  GRID_DEG: 0.025\n",
      "  Unique cells: 64\n",
      "\n",
      "Spatial raw data:\n",
      "  grid_lat range: [40.600000, 40.875000]\n",
      "  grid_lng range: [-74.100000, -73.850000]\n",
      "  grid_lat spread: 0.275000\n",
      "  grid_lng spread: 0.250000\n",
      "\n",
      "Spatial statistics:\n",
      "  lat_mean: 40.734289\n",
      "  lat_std:  0.064653  ← CHECK THIS!\n",
      "  lng_mean: -73.962372\n",
      "  lng_std:  0.041358  ← CHECK THIS!\n",
      "\n",
      "Normalized values:\n",
      "  grid_lat_norm range: [-2.08, 2.18]\n",
      "  grid_lng_norm range: [-3.33, 2.72]\n",
      "\n",
      "Bad values check:\n",
      "  grid_lat_norm: inf=0, nan=0\n",
      "  grid_lng_norm: inf=0, nan=0\n",
      "\n",
      "Polynomial features:\n",
      "  lat2 range: [0.02, 4.74]\n",
      "  lng2 range: [0.09, 11.07]\n",
      "  lat_lng range: [-1.38, 3.58]\n",
      "  lat2: inf=0, nan=0\n",
      "  lng2: inf=0, nan=0\n",
      "  lat_lng: inf=0, nan=0\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ADD THIS RIGHT BEFORE CREATING SPATIAL FEATURES:\n",
    "train_df = con.execute(f\"SELECT * FROM read_parquet('{grid_train_path.as_posix()}')\").fetch_df()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPATIAL FEATURES DEBUG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nGrid configuration:\")\n",
    "print(f\"  GRID_DEG: {GRID_DEG}\")\n",
    "print(f\"  Unique cells: {train_df['cell_id'].nunique() if 'cell_id' in train_df.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nSpatial raw data:\")\n",
    "print(f\"  grid_lat range: [{train_df['grid_lat'].min():.6f}, {train_df['grid_lat'].max():.6f}]\")\n",
    "print(f\"  grid_lng range: [{train_df['grid_lng'].min():.6f}, {train_df['grid_lng'].max():.6f}]\")\n",
    "print(f\"  grid_lat spread: {train_df['grid_lat'].max() - train_df['grid_lat'].min():.6f}\")\n",
    "print(f\"  grid_lng spread: {train_df['grid_lng'].max() - train_df['grid_lng'].min():.6f}\")\n",
    "\n",
    "lat_mean = float(train_df[\"grid_lat\"].mean())\n",
    "lat_std = float(train_df[\"grid_lat\"].std())\n",
    "lng_mean = float(train_df[\"grid_lng\"].mean())\n",
    "lng_std = float(train_df[\"grid_lng\"].std())\n",
    "\n",
    "print(f\"\\nSpatial statistics:\")\n",
    "print(f\"  lat_mean: {lat_mean:.6f}\")\n",
    "print(f\"  lat_std:  {lat_std:.6f}  ← CHECK THIS!\")\n",
    "print(f\"  lng_mean: {lng_mean:.6f}\")\n",
    "print(f\"  lng_std:  {lng_std:.6f}  ← CHECK THIS!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL: Check for near-zero std!\n",
    "# ============================================================================\n",
    "if lat_std < 0.001 or lng_std < 0.001:\n",
    "    print(f\"\\n WARNING: STD TOO SMALL!\")\n",
    "    print(f\"   This will cause EXTREME values after normalization!\")\n",
    "    print(f\"   Division by {lat_std:.6f} or {lng_std:.6f} → Inf/NaN!\")\n",
    "    \n",
    "# Try normalizing and check results\n",
    "train_df[\"grid_lat_norm\"] = (train_df[\"grid_lat\"] - lat_mean) / lat_std\n",
    "train_df[\"grid_lng_norm\"] = (train_df[\"grid_lng\"] - lng_mean) / lng_std\n",
    "\n",
    "print(f\"\\nNormalized values:\")\n",
    "print(f\"  grid_lat_norm range: [{train_df['grid_lat_norm'].min():.2f}, {train_df['grid_lat_norm'].max():.2f}]\")\n",
    "print(f\"  grid_lng_norm range: [{train_df['grid_lng_norm'].min():.2f}, {train_df['grid_lng_norm'].max():.2f}]\")\n",
    "\n",
    "# Check for bad values\n",
    "n_inf_lat = np.isinf(train_df['grid_lat_norm']).sum()\n",
    "n_nan_lat = train_df['grid_lat_norm'].isna().sum()\n",
    "n_inf_lng = np.isinf(train_df['grid_lng_norm']).sum()\n",
    "n_nan_lng = train_df['grid_lng_norm'].isna().sum()\n",
    "\n",
    "print(f\"\\nBad values check:\")\n",
    "print(f\"  grid_lat_norm: inf={n_inf_lat}, nan={n_nan_lat}\")\n",
    "print(f\"  grid_lng_norm: inf={n_inf_lng}, nan={n_nan_lng}\")\n",
    "\n",
    "if n_inf_lat > 0 or n_nan_lat > 0 or n_inf_lng > 0 or n_nan_lng > 0:\n",
    "    print(f\"\\nBAD VALUES FOUND! THIS WILL CRASH!\")\n",
    "\n",
    "# Create polynomial features\n",
    "train_df[\"lat2\"] = train_df[\"grid_lat_norm\"]**2\n",
    "train_df[\"lng2\"] = train_df[\"grid_lng_norm\"]**2\n",
    "train_df[\"lat_lng\"] = train_df[\"grid_lat_norm\"] * train_df[\"grid_lng_norm\"]\n",
    "\n",
    "print(f\"\\nPolynomial features:\")\n",
    "print(f\"  lat2 range: [{train_df['lat2'].min():.2f}, {train_df['lat2'].max():.2f}]\")\n",
    "print(f\"  lng2 range: [{train_df['lng2'].min():.2f}, {train_df['lng2'].max():.2f}]\")\n",
    "print(f\"  lat_lng range: [{train_df['lat_lng'].min():.2f}, {train_df['lat_lng'].max():.2f}]\")\n",
    "\n",
    "# Check polynomials for bad values\n",
    "for col in [\"lat2\", \"lng2\", \"lat_lng\"]:\n",
    "    n_inf = np.isinf(train_df[col]).sum()\n",
    "    n_nan = train_df[col].isna().sum()\n",
    "    print(f\"  {col}: inf={n_inf}, nan={n_nan}\")\n",
    "    if n_inf > 0 or n_nan > 0:\n",
    "        print(f\"  BAD VALUES IN {col}!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/grid_2025_cell_hour_bike_all.parquet\n",
      "        n\n",
      "0  412528\n"
     ]
    }
   ],
   "source": [
    "grid_2025_path = OUT_DIR / \"grid_2025_cell_hour_bike_all.parquet\"\n",
    "\n",
    "if FORCE_REBUILD and grid_2025_path.exists():\n",
    "    grid_2025_path.unlink()\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "COPY (\n",
    "  WITH e AS (\n",
    "    SELECT\n",
    "      hour_ts, cell_id, grid_lat, grid_lng, exposure_min\n",
    "    FROM read_parquet('{exposure_cell_hour_path.as_posix()}')\n",
    "    WHERE hour_ts >= TIMESTAMP '2025-01-01'\n",
    "      AND hour_ts <  TIMESTAMP '2026-01-01'\n",
    "      AND exposure_min > 0\n",
    "  ),\n",
    "  e_keep AS (\n",
    "    SELECT e.*\n",
    "    FROM e\n",
    "    INNER JOIN read_parquet('{cells_keep_path.as_posix()}') k USING(cell_id)\n",
    "  )\n",
    "  SELECT\n",
    "    e_keep.hour_ts,\n",
    "    e_keep.cell_id,\n",
    "    e_keep.grid_lat,\n",
    "    e_keep.grid_lng,\n",
    "    e_keep.exposure_min,\n",
    "    EXTRACT(HOUR  FROM e_keep.hour_ts) AS hour_of_day,\n",
    "    EXTRACT(DOW   FROM e_keep.hour_ts) AS dow,\n",
    "    EXTRACT(MONTH FROM e_keep.hour_ts) AS month,\n",
    "    w.temp, w.prcp, w.snow, w.wspd\n",
    "  FROM e_keep\n",
    "  LEFT JOIN weather_hourly w USING(hour_ts)\n",
    "  ORDER BY hour_ts, cell_id\n",
    ")\n",
    "TO '{grid_2025_path.as_posix()}'\n",
    "(FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Wrote:\", grid_2025_path)\n",
    "print(con.execute(f\"SELECT COUNT(*) n FROM read_parquet('{grid_2025_path.as_posix()}')\").fetch_df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING YEAR-LEVEL WEATHER BOOTSTRAP (day-of-year mapping)\n",
      "======================================================================\n",
      "  Year 2020: 8734 unique (dayofyear, hour) combinations\n",
      "  Year 2021: 8709 unique (dayofyear, hour) combinations\n",
      "  Year 2022: 8760 unique (dayofyear, hour) combinations\n",
      "  Year 2023: 8760 unique (dayofyear, hour) combinations\n",
      "  Year 2024: 8784 unique (dayofyear, hour) combinations\n",
      "\n",
      "Day 15, hour 14 across years:\n",
      "  2020: temp=+5.6°C, prcp=0.0mm\n",
      "  2021: temp=+0.9°C, prcp=0.0mm\n",
      "  2022: temp=-10.6°C, prcp=0.0mm\n",
      "  2023: temp=-1.6°C, prcp=0.0mm\n",
      "  2024: temp=-5.3°C, prcp=0.0mm\n",
      "\n",
      "Prepared 2025 data: 412528 rows\n",
      "\n",
      "======================================================================\n",
      "RUNNING YEAR-LEVEL WEATHER BOOTSTRAP (day-of-year mapping)\n",
      "======================================================================\n",
      "\n",
      "Model: Poisson\n",
      "  Simulation 0/500...\n",
      "  Simulation 50/500...\n",
      "  Simulation 100/500...\n",
      "  Simulation 150/500...\n",
      "  Simulation 200/500...\n",
      "  Simulation 250/500...\n",
      "  Simulation 300/500...\n",
      "  Simulation 350/500...\n",
      "  Simulation 400/500...\n",
      "  Simulation 450/500...\n",
      "\n",
      "Model: Negative Binomial\n",
      "  Simulation 0/500...\n",
      "  Simulation 50/500...\n",
      "  Simulation 100/500...\n",
      "  Simulation 150/500...\n",
      "  Simulation 200/500...\n",
      "  Simulation 250/500...\n",
      "  Simulation 300/500...\n",
      "  Simulation 350/500...\n",
      "  Simulation 400/500...\n",
      "  Simulation 450/500...\n",
      "\n",
      "======================================================================\n",
      "RESULTS WITH YEAR-LEVEL WEATHER BOOTSTRAP\n",
      "======================================================================\n",
      "Poisson: 5th = 6419, Median = 6575, 95th = 6745\n",
      "NegBin:  5th = 7114, Median = 7350, 95th = 7606\n",
      "\n",
      "Poisson 90% CI: [6419, 6745], width = 326 (5.0%)\n",
      "NegBin  90% CI: [7114, 7606], width = 492 (6.7%)\n",
      "\n",
      "Observed 2025: 6,728\n",
      "NegBin coverage: ❌ NO\n",
      "\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/risk_mc_2025_totals_bike_all.parquet\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/poisson_bike_all_params.parquet /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/poisson_bike_all_cov.parquet\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/negbin_bike_all_params.parquet /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/negbin_bike_all_cov.parquet\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/model_comparison_bike_all.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>aic</th>\n",
       "      <th>dispersion</th>\n",
       "      <th>q50_total_2025</th>\n",
       "      <th>q05_total_2025</th>\n",
       "      <th>q95_total_2025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poisson</td>\n",
       "      <td>240602.006487</td>\n",
       "      <td>1.451729</td>\n",
       "      <td>6575.0</td>\n",
       "      <td>6418.95</td>\n",
       "      <td>6745.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg_bin</td>\n",
       "      <td>250526.526426</td>\n",
       "      <td>1.041182</td>\n",
       "      <td>7350.0</td>\n",
       "      <td>7113.95</td>\n",
       "      <td>7606.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model            aic  dispersion  q50_total_2025  q05_total_2025  \\\n",
       "0  poisson  240602.006487    1.451729          6575.0         6418.95   \n",
       "1  neg_bin  250526.526426    1.041182          7350.0         7113.95   \n",
       "\n",
       "   q95_total_2025  \n",
       "0         6745.05  \n",
       "1         7606.30  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patsy\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(42)\n",
    "S = 500  # 5 years × 100 iterations each\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Weather for ALL Years (by day-of-year + hour)\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING YEAR-LEVEL WEATHER BOOTSTRAP (day-of-year mapping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load training weather\n",
    "train_weather = con.execute(f\"SELECT * FROM read_parquet('{grid_train_path.as_posix()}')\").fetch_df()\n",
    "train_weather[\"hour_ts\"] = pd.to_datetime(train_weather[\"hour_ts\"])\n",
    "train_weather[\"year\"] = train_weather[\"hour_ts\"].dt.year\n",
    "train_weather[\"dayofyear\"] = train_weather[\"hour_ts\"].dt.dayofyear\n",
    "train_weather[\"hour\"] = train_weather[\"hour_ts\"].dt.hour\n",
    "\n",
    "weather_cols = [\"temp\", \"prcp\", \"snow\", \"wspd\"]\n",
    "available_years = [2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# Build lookup: year → (dayofyear, hour) → weather\n",
    "year_weather_dict = {}\n",
    "\n",
    "for year in available_years:\n",
    "    year_data = train_weather[train_weather['year'] == year].copy()\n",
    "    \n",
    "    # Aggregate to (dayofyear, hour) - take median if multiple observations\n",
    "    year_agg = year_data.groupby(['dayofyear', 'hour'])[weather_cols].median().reset_index()\n",
    "    \n",
    "    # Convert to dict\n",
    "    year_dict = {}\n",
    "    for _, row in year_agg.iterrows():\n",
    "        key = (int(row['dayofyear']), int(row['hour']))\n",
    "        year_dict[key] = {col: row[col] for col in weather_cols}\n",
    "    \n",
    "    year_weather_dict[year] = year_dict\n",
    "    print(f\"  Year {year}: {len(year_dict)} unique (dayofyear, hour) combinations\")\n",
    "\n",
    "# Check coverage\n",
    "test_key = (15, 14)  # 15th day of year, 14:00\n",
    "print(f\"\\nDay 15, hour 14 across years:\")\n",
    "for year in available_years:\n",
    "    if test_key in year_weather_dict[year]:\n",
    "        w = year_weather_dict[year][test_key]\n",
    "        print(f\"  {year}: temp={w['temp']:+.1f}°C, prcp={w['prcp']:.1f}mm\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Prepare 2025 Data\n",
    "# ============================================================================\n",
    "df2025 = con.execute(f\"SELECT * FROM read_parquet('{grid_2025_path.as_posix()}')\").fetch_df()\n",
    "df2025 = df2025.dropna(subset=[\"exposure_min\",\"hour_of_day\",\"dow\",\"month\",\"grid_lat\",\"grid_lng\"]).copy()\n",
    "df2025 = df2025[df2025[\"exposure_min\"] > 0].copy()\n",
    "\n",
    "df2025[\"hour_ts\"] = pd.to_datetime(df2025[\"hour_ts\"])\n",
    "df2025[\"dayofyear\"] = df2025[\"hour_ts\"].dt.dayofyear.astype(int)\n",
    "df2025[\"hour\"] = df2025[\"hour_ts\"].dt.hour.astype(int)\n",
    "\n",
    "df2025[\"hour_of_day\"] = df2025[\"hour_of_day\"].astype(int).astype(\"category\")\n",
    "df2025[\"dow\"] = df2025[\"dow\"].astype(int).astype(\"category\")\n",
    "df2025[\"month\"] = df2025[\"month\"].astype(int).astype(\"category\")\n",
    "\n",
    "if meta_path.exists():\n",
    "    meta = json.loads(meta_path.read_text())\n",
    "    wstats = meta[\"weather_stats\"]\n",
    "    sstats = meta[\"spatial_stats\"]\n",
    "else:\n",
    "    raise ValueError(\"meta_path not found!\")\n",
    "\n",
    "print(f\"\\nPrepared 2025 data: {len(df2025)} rows\")\n",
    "\n",
    "# Pre-extract keys\n",
    "df2025_keys = list(zip(df2025['dayofyear'], df2025['hour']))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Simulation with Day-of-Year Weather Mapping\n",
    "# ============================================================================\n",
    "def simulate_totals_year_weather(res, use_nb: bool):\n",
    "    \"\"\"\n",
    "    For each simulation: Pick ONE year and map 2025 dates to that year's weather\n",
    "    \"\"\"\n",
    "    design_info = res.model.data.design_info\n",
    "    alpha_nb = float(res.model.family.alpha) if use_nb else 0.0\n",
    "    \n",
    "    totals = np.zeros(S, dtype=np.int64)\n",
    "    \n",
    "    for s in range(S):\n",
    "        if s % 50 == 0:\n",
    "            print(f\"  Simulation {s}/{S}...\")\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # A) Pick ONE year's weather\n",
    "        # ----------------------------------------------------------------\n",
    "        sampled_year = rng.choice(available_years)\n",
    "        year_weather = year_weather_dict[sampled_year]\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # B) Map 2025 (dayofyear, hour) → sampled year weather\n",
    "        # ----------------------------------------------------------------\n",
    "        weather_matrix = np.zeros((len(df2025), 4))\n",
    "        \n",
    "        for i, key in enumerate(df2025_keys):\n",
    "            if key in year_weather:\n",
    "                w = year_weather[key]\n",
    "                weather_matrix[i] = [w['temp'], w['prcp'], w['snow'], w['wspd']]\n",
    "            # else: keep 0 (missing weather)\n",
    "        \n",
    "        # Apply to df_sim\n",
    "        df_sim = df2025.copy()\n",
    "        df_sim['temp'] = weather_matrix[:, 0]\n",
    "        df_sim['prcp'] = weather_matrix[:, 1]\n",
    "        df_sim['snow'] = weather_matrix[:, 2]\n",
    "        df_sim['wspd'] = weather_matrix[:, 3]\n",
    "        \n",
    "        # Scale weather\n",
    "        for col in weather_cols:\n",
    "            if col in wstats:\n",
    "                df_sim[col] = (df_sim[col] - wstats[col][\"mean\"]) / wstats[col][\"std\"]\n",
    "        \n",
    "        # Scale spatial\n",
    "        df_sim[\"grid_lat_norm\"] = (df_sim[\"grid_lat\"] - sstats[\"lat_mean\"]) / sstats[\"lat_std\"]\n",
    "        df_sim[\"grid_lng_norm\"] = (df_sim[\"grid_lng\"] - sstats[\"lng_mean\"]) / sstats[\"lng_std\"]\n",
    "        df_sim[\"lat2\"] = df_sim[\"grid_lat_norm\"]**2\n",
    "        df_sim[\"lng2\"] = df_sim[\"grid_lng_norm\"]**2\n",
    "        df_sim[\"lat_lng\"] = df_sim[\"grid_lat_norm\"] * df_sim[\"grid_lng_norm\"]\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # C) Sample β\n",
    "        # ----------------------------------------------------------------\n",
    "        X = patsy.build_design_matrices([design_info], df_sim, return_type=\"dataframe\")[0]\n",
    "        xcols = X.columns.tolist()\n",
    "        \n",
    "        beta_hat = res.params.loc[xcols].values\n",
    "        cov_hat = res.cov_params().loc[xcols, xcols].values\n",
    "        beta_s = rng.multivariate_normal(mean=beta_hat, cov=cov_hat)\n",
    "        \n",
    "        # ----------------------------------------------------------------\n",
    "        # D) Calculate μ and Sample Crashes\n",
    "        # ----------------------------------------------------------------\n",
    "        df_aligned = df_sim.loc[X.index]\n",
    "        E = df_aligned[\"exposure_min\"].values\n",
    "        \n",
    "        eta = X.values @ beta_s\n",
    "        eta = np.clip(eta, -20, 20)\n",
    "        mu = E * np.exp(eta)\n",
    "        mu = np.clip(mu, 0, 1e6)\n",
    "        \n",
    "        if use_nb and alpha_nb > 0:\n",
    "            shape = 1.0 / alpha_nb\n",
    "            lam = rng.gamma(shape=shape, scale=alpha_nb * mu)\n",
    "            lam = np.clip(lam, 0, 1e6)\n",
    "            y = rng.poisson(lam)\n",
    "        else:\n",
    "            y = rng.poisson(mu)\n",
    "        \n",
    "        totals[s] = int(y.sum())\n",
    "    \n",
    "    return totals\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Run Simulations\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING YEAR-LEVEL WEATHER BOOTSTRAP (day-of-year mapping)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nModel: Poisson\")\n",
    "tot_p = simulate_totals_year_weather(poisson_res, use_nb=False)\n",
    "\n",
    "print(\"\\nModel: Negative Binomial\")\n",
    "tot_n = simulate_totals_year_weather(nb_res, use_nb=True)\n",
    "\n",
    "def summarize(arr):\n",
    "    q05, q50, q95 = np.quantile(arr, [0.05, 0.5, 0.95])\n",
    "    return float(q05), float(q50), float(q95)\n",
    "\n",
    "p05, p50, p95 = summarize(tot_p)\n",
    "n05, n50, n95 = summarize(tot_n)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS WITH YEAR-LEVEL WEATHER BOOTSTRAP\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Poisson: 5th = {p05:.0f}, Median = {p50:.0f}, 95th = {p95:.0f}\")\n",
    "print(f\"NegBin:  5th = {n05:.0f}, Median = {n50:.0f}, 95th = {n95:.0f}\")\n",
    "print(f\"\\nPoisson 90% CI: [{p05:.0f}, {p95:.0f}], width = {p95-p05:.0f} ({(p95-p05)/p50*100:.1f}%)\")\n",
    "print(f\"NegBin  90% CI: [{n05:.0f}, {n95:.0f}], width = {n95-n05:.0f} ({(n95-n05)/n50*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# ---- Save ----\n",
    "mc_df = pd.DataFrame({\n",
    "    \"model\": [\"poisson\"]*S + [\"neg_bin\"]*S,\n",
    "    \"total_2025\": np.concatenate([tot_p, tot_n])\n",
    "})\n",
    "mc_path = OUT_DIR / \"risk_mc_2025_totals_bike_all.parquet\"\n",
    "mc_df.to_parquet(mc_path, index=False)\n",
    "print(\"\\nSaved:\", mc_path)\n",
    "\n",
    "def save_model_artifacts(res, model_name: str):\n",
    "    params_path = OUT_DIR / f\"{model_name}_params.parquet\"\n",
    "    cov_path    = OUT_DIR / f\"{model_name}_cov.parquet\"\n",
    "    res.params.to_frame(\"coef\").reset_index(names=\"term\").to_parquet(params_path, index=False)\n",
    "    res.cov_params().reset_index().to_parquet(cov_path, index=False)\n",
    "    return params_path, cov_path\n",
    "\n",
    "p_params, p_cov = save_model_artifacts(poisson_res, \"poisson_bike_all\")\n",
    "n_params, n_cov = save_model_artifacts(nb_res, \"negbin_bike_all\")\n",
    "\n",
    "print(\"Saved:\", p_params, p_cov)\n",
    "print(\"Saved:\", n_params, n_cov)\n",
    "\n",
    "cmp = pd.DataFrame([\n",
    "    {\"model\":\"poisson\", \"aic\": float(poisson_res.aic), \"dispersion\": float(np.sum(poisson_res.resid_pearson**2)/poisson_res.df_resid),\n",
    "     \"q50_total_2025\": p50, \"q05_total_2025\": p05, \"q95_total_2025\": p95},\n",
    "    {\"model\":\"neg_bin\", \"aic\": float(nb_res.aic), \"dispersion\": float(np.sum(nb_res.resid_pearson**2)/nb_res.df_resid),\n",
    "     \"q50_total_2025\": n50, \"q05_total_2025\": n05, \"q95_total_2025\": n95},\n",
    "])\n",
    "cmp_path = OUT_DIR / \"model_comparison_bike_all.parquet\"\n",
    "cmp.to_parquet(cmp_path, index=False)\n",
    "print(\"Saved:\", cmp_path)\n",
    "\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2025 prepared with all features\n",
      "Saved: /Users/patricknussbaum/Desktop/projects/city-bike/data/processed/risk_hourly_mc/risk_eval_2025_monthly_bike_all.parquet\n",
      "\n",
      "Preview:\n",
      "    month_ts  observed   pred_mean    model\n",
      "0 2025-01-01     291.0  328.618174  poisson\n",
      "1 2025-02-01     302.0  317.416231  poisson\n",
      "2 2025-03-01     508.0  519.185383  poisson\n",
      "3 2025-04-01     546.0  544.403365  poisson\n",
      "4 2025-05-01     736.0  646.118406  poisson\n",
      "5 2025-06-01     683.0  663.116768  poisson\n",
      "6 2025-07-01     776.0  702.470406  poisson\n",
      "7 2025-08-01     758.0  755.293659  poisson\n",
      "8 2025-09-01     713.0  779.961723  poisson\n",
      "9 2025-10-01     688.0  702.694071  poisson\n",
      "\n",
      "Wide format (for reference):\n",
      "     month_ts  y_obs  y_pred_poisson  y_pred_negbin\n",
      "0  2025-01-01  291.0      328.618174     351.736641\n",
      "1  2025-02-01  302.0      317.416231     343.931734\n",
      "2  2025-03-01  508.0      519.185383     577.078929\n",
      "3  2025-04-01  546.0      544.403365     609.552238\n",
      "4  2025-05-01  736.0      646.118406     741.045104\n",
      "5  2025-06-01  683.0      663.116768     745.648545\n",
      "6  2025-07-01  776.0      702.470406     771.572831\n",
      "7  2025-08-01  758.0      755.293659     831.761193\n",
      "8  2025-09-01  713.0      779.961723     880.573560\n",
      "9  2025-10-01  688.0      702.694071     793.190614\n",
      "10 2025-11-01  550.0      518.336236     583.469168\n",
      "11 2025-12-01  246.0             NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "# Observed crashes 2025 (citywide)\n",
    "obs = con.execute(f\"\"\"\n",
    "SELECT\n",
    "  date_trunc('month', hour_ts) AS month_ts,\n",
    "  SUM(y_bike) AS y_obs\n",
    "FROM read_parquet('{crashes_citywide_path.as_posix()}')\n",
    "WHERE hour_ts >= TIMESTAMP '2025-01-01'\n",
    "  AND hour_ts <  TIMESTAMP '2026-01-01'\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\"\"\").fetch_df()\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare df2025 with ALL features for expected_monthly\n",
    "# ============================================================================\n",
    "# df2025 already loaded in previous cell, but needs all derived features\n",
    "\n",
    "# Apply weather scaling\n",
    "for col in weather_cols:\n",
    "    if col in wstats:\n",
    "        df2025[col] = (df2025[col] - wstats[col][\"mean\"]) / wstats[col][\"std\"]\n",
    "\n",
    "# Apply spatial scaling\n",
    "df2025[\"grid_lat_norm\"] = (df2025[\"grid_lat\"] - sstats[\"lat_mean\"]) / sstats[\"lat_std\"]\n",
    "df2025[\"grid_lng_norm\"] = (df2025[\"grid_lng\"] - sstats[\"lng_mean\"]) / sstats[\"lng_std\"]\n",
    "\n",
    "# Spatial features\n",
    "df2025[\"lat2\"] = df2025[\"grid_lat_norm\"]**2\n",
    "df2025[\"lng2\"] = df2025[\"grid_lng_norm\"]**2\n",
    "df2025[\"lat_lng\"] = df2025[\"grid_lat_norm\"] * df2025[\"grid_lng_norm\"]\n",
    "\n",
    "print(\"df2025 prepared with all features\")\n",
    "\n",
    "# ============================================================================\n",
    "# Expected from grid-model: sum over cells of mu_cellhour\n",
    "# ============================================================================\n",
    "def expected_monthly(res, label: str):\n",
    "    import patsy\n",
    "    design_info = res.model.data.design_info\n",
    "    X = patsy.build_design_matrices([design_info], df2025, return_type=\"dataframe\")[0]\n",
    "    \n",
    "    # Align to X's index\n",
    "    df_aligned = df2025.loc[X.index]\n",
    "    \n",
    "    beta = res.params.loc[X.columns].values\n",
    "    eta = X.values @ beta\n",
    "    \n",
    "    # Clip for safety\n",
    "    eta = np.clip(eta, -20, 20)\n",
    "    \n",
    "    mu = df_aligned[\"exposure_min\"].values * np.exp(eta)\n",
    "    \n",
    "    tmp = pd.DataFrame({\"hour_ts\": df_aligned[\"hour_ts\"].values, \"mu\": mu})\n",
    "    tmp[\"month_ts\"] = pd.to_datetime(tmp[\"hour_ts\"]).dt.to_period('M').dt.to_timestamp()\n",
    "    out = tmp.groupby(\"month_ts\", as_index=False)[\"mu\"].sum()\n",
    "    out = out.rename(columns={\"mu\": f\"y_pred_{label}\"})\n",
    "    return out\n",
    "\n",
    "pred_p = expected_monthly(poisson_res, \"poisson\")\n",
    "pred_n = expected_monthly(nb_res, \"negbin\")\n",
    "\n",
    "# Merge predictions (wide format first)\n",
    "eval_wide = obs.merge(pred_p, on=\"month_ts\", how=\"left\").merge(pred_n, on=\"month_ts\", how=\"left\")\n",
    "\n",
    "# Convert to LONG format for dashboard\n",
    "eval_long_poisson = eval_wide[[\"month_ts\", \"y_obs\", \"y_pred_poisson\"]].copy()\n",
    "eval_long_poisson[\"model\"] = \"poisson\"\n",
    "eval_long_poisson = eval_long_poisson.rename(columns={\"y_obs\": \"observed\", \"y_pred_poisson\": \"pred_mean\"})\n",
    "\n",
    "eval_long_nb = eval_wide[[\"month_ts\", \"y_obs\", \"y_pred_negbin\"]].copy()\n",
    "eval_long_nb[\"model\"] = \"neg_bin\"\n",
    "eval_long_nb = eval_long_nb.rename(columns={\"y_obs\": \"observed\", \"y_pred_negbin\": \"pred_mean\"})\n",
    "\n",
    "# Combine\n",
    "eval_df = pd.concat([eval_long_poisson, eval_long_nb], ignore_index=True)\n",
    "\n",
    "# Save\n",
    "eval_path = OUT_DIR / \"risk_eval_2025_monthly_bike_all.parquet\"\n",
    "eval_df.to_parquet(eval_path, index=False)\n",
    "\n",
    "print(\"Saved:\", eval_path)\n",
    "print(\"\\nPreview:\")\n",
    "print(eval_df.head(10))\n",
    "\n",
    "# Show wide format too for inspection\n",
    "print(\"\\nWide format (for reference):\")\n",
    "print(eval_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "city-bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
